{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Model Selection and Parameter Tuning\n\nThis notebook demonstrates key capabilities of the measurement framework using an **A/A test** — treatment labels are assigned randomly to 50% of products with no real intervention. The true treatment effect is 0 by construction.\n\n1. **Model swappability** — Given the same data, switch between cross-sectional models by overriding a single config entry.\n2. **Monte Carlo model comparison** — Run models across multiple replications to obtain sampling distributions, separating bias from variance.\n3. **Parameter sensitivity** — For a given model, investigate how tuning parameters affect the treatment effect estimate.\n4. **Parameter sensitivity with uncertainty** — Add uncertainty bands to parameter sweeps via Monte Carlo replications.\n\nAll three cross-sectional models share the same data-generation process: a single-day simulation with random treatment labels assigned to 50% of products (no real intervention).\n\n### Workflow Overview\n\n1. Generate a shared product catalog and define model overrides\n2. Loop over models and override `MEASUREMENT` in the base config, write a temp YAML, call `evaluate_impact()`\n3. Compare treatment effect estimates against the known true effect of 0\n4. Run Monte Carlo replications (varying outcome seeds, fixed treatment assignment) and visualize sampling distributions\n5. Sweep tuning parameters for subclassification and nearest neighbour matching\n6. Re-run parameter sweeps across replications and visualize uncertainty bands"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from impact_engine_measure import evaluate_impact, load_results, parse_config_file\n",
    "from online_retail_simulator import simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1 — Shared Data\n\nAll models will use the same product catalog."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_job = simulate(\"configs/demo_model_selection_catalog.yaml\", job_id=\"catalog\")\n",
    "products = catalog_job.load_df(\"products\")\n",
    "\n",
    "print(f\"Generated {len(products)} products\")\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2 — Configuration\n\nPoint to the impact engine config and set the ground truth.\n\nTreatment assignment is controlled by the config's `DATA.ENRICHMENT` section. The `product_detail_boost` function randomly assigns 50% of products to treatment (`enrichment_fraction: 0.5`). Because `quality_boost: 0.0`, there is no actual effect — this is an A/A test and the true treatment effect is 0 by construction."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "output_path = Path(\"output/demo_model_selection\")\noutput_path.mkdir(parents=True, exist_ok=True)\n\nconfig_path = \"configs/demo_model_selection.yaml\"\ntrue_te = 0  # A/A design: no treatment effect by construction"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Swappability\n",
    "\n",
    "We load one base config and override `MEASUREMENT` for each model.\n",
    "Each iteration writes a temporary YAML and calls `evaluate_impact()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_override(base_config, measurement_override, storage_url, job_id, source_seed=None):\n",
    "    \"\"\"Override MEASUREMENT in base config, write temp YAML, run evaluate_impact().\n",
    "\n",
    "    Optionally override the data-generating seed for Monte Carlo replications.\n",
    "    \"\"\"\n",
    "    config = copy.deepcopy(base_config)\n",
    "    config[\"MEASUREMENT\"] = measurement_override\n",
    "    if source_seed is not None:\n",
    "        config[\"DATA\"][\"SOURCE\"][\"CONFIG\"][\"seed\"] = source_seed\n",
    "\n",
    "    tmp_config_path = Path(storage_url) / f\"config_{job_id}.yaml\"\n",
    "    tmp_config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tmp_config_path, \"w\") as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "    job_info = evaluate_impact(str(tmp_config_path), storage_url, job_id=job_id)\n",
    "    result = load_results(job_info)\n",
    "    return result.impact_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = parse_config_file(config_path)\n",
    "\n",
    "model_overrides = {\n",
    "    \"Experiment (OLS)\": {\n",
    "        \"MODEL\": \"experiment\",\n",
    "        \"PARAMS\": {\"formula\": \"revenue ~ enriched + price\"},\n",
    "    },\n",
    "    \"Subclassification\": {\n",
    "        \"MODEL\": \"subclassification\",\n",
    "        \"PARAMS\": {\n",
    "            \"treatment_column\": \"enriched\",\n",
    "            \"covariate_columns\": [\"price\"],\n",
    "            \"n_strata\": 5,\n",
    "            \"estimand\": \"att\",\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "        },\n",
    "    },\n",
    "    \"Nearest Neighbour Matching\": {\n",
    "        \"MODEL\": \"nearest_neighbour_matching\",\n",
    "        \"PARAMS\": {\n",
    "            \"treatment_column\": \"enriched\",\n",
    "            \"covariate_columns\": [\"price\"],\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "            \"caliper\": 0.2,\n",
    "            \"replace\": True,\n",
    "            \"ratio\": 1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def extract_te(results):\n",
    "    \"\"\"Extract the treatment effect from model results regardless of model type.\"\"\"\n",
    "    estimates = results[\"data\"][\"impact_estimates\"]\n",
    "    model_type = results[\"model_type\"]\n",
    "    if model_type == \"experiment\":\n",
    "        return estimates[\"params\"].get(\"enriched[T.True]\", estimates[\"params\"].get(\"enriched\", 0))\n",
    "    elif model_type == \"nearest_neighbour_matching\":\n",
    "        return estimates[\"att\"]\n",
    "    else:\n",
    "        return estimates[\"treatment_effect\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "model_estimates = {}\n",
    "\n",
    "for name, measurement in model_overrides.items():\n",
    "    job_id = measurement[\"MODEL\"]\n",
    "    results = run_with_override(base_config, measurement, str(output_path), job_id)\n",
    "    model_results[name] = results\n",
    "    model_estimates[name] = extract_te(results)\n",
    "    print(f\"{name}: treatment effect = {model_estimates[name]:.4f}\")\n",
    "\n",
    "print(f\"\\nTrue effect: {true_te:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Estimate\": est,\n",
    "            \"True Effect\": true_te,\n",
    "            \"Absolute Error\": abs(est - true_te),\n",
    "        }\n",
    "        for name, est in model_estimates.items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CROSS-SECTIONAL MODEL COMPARISON (A/A)\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_support import plot_model_comparison\n",
    "\n",
    "plot_model_comparison(\n",
    "    model_names=list(model_estimates.keys()),\n",
    "    estimates=list(model_estimates.values()),\n",
    "    true_effect=true_te,\n",
    "    ylabel=\"Treatment Effect\",\n",
    "    title=\"A/A Test: Model Estimates (True Effect = 0)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\n\nN_REPS = 10\nrng = np.random.default_rng(seed=2024)\nmc_seeds = rng.integers(low=0, high=2**31, size=N_REPS).tolist()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Monte Carlo Model Comparison\n\nPart 1 used a single random seed, making it impossible to distinguish systematic bias from sampling noise. Here we run all three models across multiple replications with varying outcome seeds to obtain sampling distributions. In the A/A setting, all models should be centered around 0.\n\n**Design**: We vary `DATA.SOURCE.CONFIG.seed` (outcome noise) while keeping `DATA.ENRICHMENT.PARAMS.seed` fixed (same treatment assignment). This isolates estimator sampling variability from treatment assignment variability."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_results = {name: [] for name in model_overrides}\n",
    "\n",
    "for i, seed in enumerate(mc_seeds):\n",
    "    for name, measurement in model_overrides.items():\n",
    "        job_id = f\"mc_{measurement['MODEL']}_rep{i}\"\n",
    "        results = run_with_override(\n",
    "            base_config,\n",
    "            measurement,\n",
    "            str(output_path),\n",
    "            job_id,\n",
    "            source_seed=seed,\n",
    "        )\n",
    "        mc_results[name].append(extract_te(results))\n",
    "\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Completed {i + 1}/{N_REPS} replications\")\n",
    "\n",
    "print(f\"Monte Carlo simulation complete: {N_REPS} replications x {len(model_overrides)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_summary = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Mean\": np.mean(estimates),\n",
    "            \"Std\": np.std(estimates, ddof=1),\n",
    "            \"Bias\": np.mean(estimates) - true_te,\n",
    "            \"RMSE\": np.sqrt(np.mean([(e - true_te) ** 2 for e in estimates])),\n",
    "            \"Min\": np.min(estimates),\n",
    "            \"Max\": np.max(estimates),\n",
    "        }\n",
    "        for name, estimates in mc_results.items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"MONTE CARLO MODEL COMPARISON ({N_REPS} replications)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"True treatment effect: {true_te:.4f}\")\n",
    "print(\"-\" * 90)\n",
    "print(mc_summary.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_support import plot_monte_carlo_distribution\n",
    "\n",
    "plot_monte_carlo_distribution(\n",
    "    model_names=list(mc_results.keys()),\n",
    "    distributions=mc_results,\n",
    "    true_effect=true_te,\n",
    "    ylabel=\"Treatment Effect\",\n",
    "    title=f\"A/A Monte Carlo Model Comparison ({N_REPS} replications)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parameter Sensitivity\n",
    "\n",
    "For a given model and data, how sensitive is the treatment effect estimate to tuning parameters?\n",
    "We use the same override pattern, varying one parameter at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Subclassification: `n_strata`\n",
    "\n",
    "More strata means finer partitioning of the covariate space.\n",
    "This can improve precision but may leave strata without common support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_strata_values = [2, 3, 5, 10, 20, 50, 100]\n",
    "subclass_estimates = []\n",
    "strata_used = []\n",
    "strata_dropped = []\n",
    "\n",
    "for n in n_strata_values:\n",
    "    measurement = {\n",
    "        \"MODEL\": \"subclassification\",\n",
    "        \"PARAMS\": {\n",
    "            \"treatment_column\": \"enriched\",\n",
    "            \"covariate_columns\": [\"price\"],\n",
    "            \"n_strata\": n,\n",
    "            \"estimand\": \"att\",\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "        },\n",
    "    }\n",
    "    results = run_with_override(base_config, measurement, str(output_path), f\"subclass_strata_{n}\")\n",
    "    estimates = results[\"data\"][\"impact_estimates\"]\n",
    "\n",
    "    subclass_estimates.append(estimates[\"treatment_effect\"])\n",
    "    strata_used.append(estimates[\"n_strata\"])\n",
    "    strata_dropped.append(estimates[\"n_strata_dropped\"])\n",
    "\n",
    "subclass_sensitivity = pd.DataFrame(\n",
    "    {\n",
    "        \"n_strata (requested)\": n_strata_values,\n",
    "        \"Strata Used\": strata_used,\n",
    "        \"Strata Dropped\": strata_dropped,\n",
    "        \"Treatment Effect\": subclass_estimates,\n",
    "        \"Absolute Error\": [abs(est - true_te) for est in subclass_estimates],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Subclassification: n_strata Sensitivity\")\n",
    "print(\"-\" * 70)\n",
    "print(subclass_sensitivity.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_support import plot_parameter_sensitivity\n",
    "\n",
    "plot_parameter_sensitivity(\n",
    "    param_values=n_strata_values,\n",
    "    estimates=subclass_estimates,\n",
    "    true_effect=true_te,\n",
    "    xlabel=\"Number of Strata (n_strata)\",\n",
    "    ylabel=\"Treatment Effect\",\n",
    "    title=\"Subclassification: Sensitivity to n_strata\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Nearest Neighbour Matching: `caliper`\n",
    "\n",
    "The caliper controls the maximum allowed distance between a treated unit and its matched control.\n",
    "Smaller values enforce tighter matches but may discard units, while larger values allow more matches with worse balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caliper_values = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0]\n",
    "matching_estimates = []\n",
    "n_matched_att_list = []\n",
    "\n",
    "for cal in caliper_values:\n",
    "    measurement = {\n",
    "        \"MODEL\": \"nearest_neighbour_matching\",\n",
    "        \"PARAMS\": {\n",
    "            \"treatment_column\": \"enriched\",\n",
    "            \"covariate_columns\": [\"price\"],\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "            \"caliper\": cal,\n",
    "            \"replace\": True,\n",
    "            \"ratio\": 1,\n",
    "        },\n",
    "    }\n",
    "    results = run_with_override(base_config, measurement, str(output_path), f\"matching_caliper_{cal}\")\n",
    "    estimates = results[\"data\"][\"impact_estimates\"]\n",
    "    summary = results[\"data\"][\"model_summary\"]\n",
    "\n",
    "    matching_estimates.append(estimates[\"att\"])\n",
    "    n_matched_att_list.append(summary[\"n_matched_att\"])\n",
    "\n",
    "matching_sensitivity = pd.DataFrame(\n",
    "    {\n",
    "        \"Caliper\": caliper_values,\n",
    "        \"N Matched (ATT)\": n_matched_att_list,\n",
    "        \"Treatment Effect (ATT)\": matching_estimates,\n",
    "        \"Absolute Error\": [abs(est - true_te) for est in matching_estimates],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Nearest Neighbour Matching: Caliper Sensitivity\")\n",
    "print(\"-\" * 70)\n",
    "print(matching_sensitivity.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_sensitivity(\n",
    "    param_values=caliper_values,\n",
    "    estimates=matching_estimates,\n",
    "    true_effect=true_te,\n",
    "    xlabel=\"Caliper\",\n",
    "    ylabel=\"Treatment Effect (ATT)\",\n",
    "    title=\"Nearest Neighbour Matching: Sensitivity to Caliper\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Parameter Sensitivity with Uncertainty\n",
    "\n",
    "Part 3 showed how estimates change with tuning parameters using a single seed.\n",
    "Here we add uncertainty bands by running each parameter value across multiple replications.\n",
    "This reveals whether apparent sensitivity is real or just noise.\n",
    "\n",
    "### 4a. Subclassification: `n_strata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_strata_mc = {n: [] for n in n_strata_values}\n",
    "\n",
    "for i, seed in enumerate(mc_seeds):\n",
    "    for n in n_strata_values:\n",
    "        measurement = {\n",
    "            \"MODEL\": \"subclassification\",\n",
    "            \"PARAMS\": {\n",
    "                \"treatment_column\": \"enriched\",\n",
    "                \"covariate_columns\": [\"price\"],\n",
    "                \"n_strata\": n,\n",
    "                \"estimand\": \"att\",\n",
    "                \"dependent_variable\": \"revenue\",\n",
    "            },\n",
    "        }\n",
    "        results = run_with_override(\n",
    "            base_config,\n",
    "            measurement,\n",
    "            str(output_path),\n",
    "            f\"mc_subclass_{n}_rep{i}\",\n",
    "            source_seed=seed,\n",
    "        )\n",
    "        n_strata_mc[n].append(results[\"data\"][\"impact_estimates\"][\"treatment_effect\"])\n",
    "\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Subclassification sweep: {i + 1}/{N_REPS} replications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_support import plot_parameter_sensitivity_mc\n",
    "\n",
    "strata_means = [np.mean(n_strata_mc[n]) for n in n_strata_values]\n",
    "strata_stds = [np.std(n_strata_mc[n], ddof=1) for n in n_strata_values]\n",
    "strata_lower = [m - s for m, s in zip(strata_means, strata_stds)]\n",
    "strata_upper = [m + s for m, s in zip(strata_means, strata_stds)]\n",
    "\n",
    "plot_parameter_sensitivity_mc(\n",
    "    param_values=n_strata_values,\n",
    "    mean_estimates=strata_means,\n",
    "    lower_band=strata_lower,\n",
    "    upper_band=strata_upper,\n",
    "    true_effect=true_te,\n",
    "    xlabel=\"Number of Strata (n_strata)\",\n",
    "    ylabel=\"Treatment Effect\",\n",
    "    title=f\"Subclassification: n_strata Sensitivity ({N_REPS} replications)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Nearest Neighbour Matching: `caliper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caliper_mc = {c: [] for c in caliper_values}\n",
    "\n",
    "for i, seed in enumerate(mc_seeds):\n",
    "    for cal in caliper_values:\n",
    "        measurement = {\n",
    "            \"MODEL\": \"nearest_neighbour_matching\",\n",
    "            \"PARAMS\": {\n",
    "                \"treatment_column\": \"enriched\",\n",
    "                \"covariate_columns\": [\"price\"],\n",
    "                \"dependent_variable\": \"revenue\",\n",
    "                \"caliper\": cal,\n",
    "                \"replace\": True,\n",
    "                \"ratio\": 1,\n",
    "            },\n",
    "        }\n",
    "        results = run_with_override(\n",
    "            base_config,\n",
    "            measurement,\n",
    "            str(output_path),\n",
    "            f\"mc_matching_{cal}_rep{i}\",\n",
    "            source_seed=seed,\n",
    "        )\n",
    "        caliper_mc[cal].append(results[\"data\"][\"impact_estimates\"][\"att\"])\n",
    "\n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Matching sweep: {i + 1}/{N_REPS} replications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_means = [np.mean(caliper_mc[c]) for c in caliper_values]\n",
    "cal_stds = [np.std(caliper_mc[c], ddof=1) for c in caliper_values]\n",
    "cal_lower = [m - s for m, s in zip(cal_means, cal_stds)]\n",
    "cal_upper = [m + s for m, s in zip(cal_means, cal_stds)]\n",
    "\n",
    "plot_parameter_sensitivity_mc(\n",
    "    param_values=caliper_values,\n",
    "    mean_estimates=cal_means,\n",
    "    lower_band=cal_lower,\n",
    "    upper_band=cal_upper,\n",
    "    true_effect=true_te,\n",
    "    xlabel=\"Caliper\",\n",
    "    ylabel=\"Treatment Effect (ATT)\",\n",
    "    title=f\"Nearest Neighbour Matching: Caliper Sensitivity ({N_REPS} replications)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n**A/A design.**\n- The true treatment effect is 0 by construction (treatment labels are random with no real intervention). This tests whether models correctly report no effect when there is none.\n\n**Model swappability.**\n- All three models produce estimates close to 0 from the same simulated data, confirming correct behavior under the null.\n- Switching models requires only changing the `MEASUREMENT` entry in the config.\n- The `evaluate_impact()` interface stays the same regardless of the model.\n\n**Monte Carlo analysis.**\n- Running multiple replications reveals the sampling distribution of each estimator, separating bias from variance.\n- Under the A/A design, all three models should be centered around 0. Deviations indicate bias.\n- With `N_REPS=10`, the distributions are informative but coarse. For publication-quality analysis, increase to `N_REPS >= 500`.\n\n**Parameter sensitivity.**\n- **Subclassification** is relatively stable across `n_strata` values. Very low values may under-partition, while very high values may drop strata with insufficient common support.\n- **Nearest neighbour matching** is more sensitive to `caliper`. Very small calipers may discard too many units, while very large calipers degrade match quality.\n- Parameter sensitivity plots with uncertainty bands show which apparent patterns are robust to sampling variation and which are noise."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}