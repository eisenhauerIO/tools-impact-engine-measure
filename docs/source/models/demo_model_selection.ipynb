{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Parameter Tuning\n",
    "\n",
    "This notebook demonstrates two key capabilities of the measurement framework:\n",
    "\n",
    "1. **Model swappability** — Given the same data, switch between cross-sectional models by overriding a single config entry.\n",
    "2. **Parameter sensitivity** — For a given model, investigate how tuning parameters affect the treatment effect estimate.\n",
    "\n",
    "All three cross-sectional models share the same data-generation process: a single-day simulation with `product_detail_boost` enrichment applied to 50% of products.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. Generate a shared product catalog and define model overrides\n",
    "2. Loop over models and override `MEASUREMENT` in the base config, write a temp YAML, call `evaluate_impact()`\n",
    "3. Compare treatment effect estimates against ground truth\n",
    "4. Sweep tuning parameters for subclassification and nearest neighbour matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from impact_engine_measure import evaluate_impact, load_results, parse_config_file\n",
    "from impact_engine_measure.metrics import create_metrics_manager\n",
    "from online_retail_simulator import simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 — Shared Data\n",
    "\n",
    "All models will use the same product catalog and enriched metrics.\n",
    "Differences in estimates come only from the model, not the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"output/demo_model_selection\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "job_info = simulate(\"configs/demo_model_selection_catalog.yaml\", job_id=\"catalog\")\n",
    "products = job_info.load_df(\"products\")\n",
    "\n",
    "print(f\"Generated {len(products)} products\")\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth\n",
    "\n",
    "Compute the true treatment effect by comparing enriched vs baseline (counterfactual) metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/demo_model_selection.yaml\"\n",
    "baseline_config_path = \"configs/demo_model_selection_baseline.yaml\"\n",
    "\n",
    "parsed_enriched = parse_config_file(config_path)\n",
    "enriched_manager = create_metrics_manager(parsed_enriched)\n",
    "enriched_metrics = enriched_manager.retrieve_metrics(products)\n",
    "\n",
    "parsed_baseline = parse_config_file(baseline_config_path)\n",
    "baseline_manager = create_metrics_manager(parsed_baseline)\n",
    "baseline_metrics = baseline_manager.retrieve_metrics(products)\n",
    "\n",
    "print(f\"Enriched records: {len(enriched_metrics)}\")\n",
    "print(f\"Baseline records: {len(baseline_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_true_effect(\n",
    "    baseline_metrics: pd.DataFrame,\n",
    "    enriched_metrics: pd.DataFrame,\n",
    ") -> dict:\n",
    "    \"\"\"Calculate TRUE ATT by comparing per-product revenue for treated products.\"\"\"\n",
    "    treated_ids = enriched_metrics[enriched_metrics[\"enriched\"]][\"product_id\"].unique()\n",
    "\n",
    "    enriched_treated = enriched_metrics[enriched_metrics[\"product_id\"].isin(treated_ids)]\n",
    "    baseline_treated = baseline_metrics[baseline_metrics[\"product_id\"].isin(treated_ids)]\n",
    "\n",
    "    enriched_mean = enriched_treated.groupby(\"product_id\")[\"revenue\"].mean().mean()\n",
    "    baseline_mean = baseline_treated.groupby(\"product_id\")[\"revenue\"].mean().mean()\n",
    "    treatment_effect = enriched_mean - baseline_mean\n",
    "\n",
    "    return {\n",
    "        \"enriched_mean\": float(enriched_mean),\n",
    "        \"baseline_mean\": float(baseline_mean),\n",
    "        \"treatment_effect\": float(treatment_effect),\n",
    "    }\n",
    "\n",
    "\n",
    "true_effect = calculate_true_effect(baseline_metrics, enriched_metrics)\n",
    "true_te = true_effect[\"treatment_effect\"]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRUE TREATMENT EFFECT (GROUND TRUTH)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Enriched mean revenue:  {true_effect['enriched_mean']:.4f}\")\n",
    "print(f\"Baseline mean revenue:  {true_effect['baseline_mean']:.4f}\")\n",
    "print(f\"True treatment effect:  {true_te:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Model Swappability\n",
    "\n",
    "We load one base config and override `MEASUREMENT` for each model.\n",
    "Each iteration writes a temporary YAML and calls `evaluate_impact()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_override(base_config, measurement_override, storage_url, job_id):\n",
    "    \"\"\"Override MEASUREMENT in base config, write temp YAML, run evaluate_impact().\"\"\"\n",
    "    config = copy.deepcopy(base_config)\n",
    "    config[\"MEASUREMENT\"] = measurement_override\n",
    "\n",
    "    tmp_config_path = Path(storage_url) / f\"config_{job_id}.yaml\"\n",
    "    tmp_config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tmp_config_path, \"w\") as f:\n",
    "        yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "    job_info = evaluate_impact(str(tmp_config_path), storage_url, job_id=job_id)\n",
    "    result = load_results(job_info)\n",
    "    return result.impact_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = parse_config_file(config_path)\n",
    "\n",
    "model_overrides = {\n",
    "    \"Experiment (OLS)\": {\n",
    "        \"MODEL\": \"experiment\",\n",
    "        \"PARAMS\": {\"formula\": \"revenue ~ enriched + price\"},\n",
    "    },\n",
    "    \"Subclassification\": {\n",
    "        \"MODEL\": \"subclassification\",\n",
    "        \"PARAMS\": {\n",
    "            \"treatment_column\": \"enriched\",\n",
    "            \"covariate_columns\": [\"price\"],\n",
    "            \"n_strata\": 5,\n",
    "            \"estimand\": \"att\",\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "        },\n",
    "    },\n",
    "    \"Nearest Neighbour Matching\": {\n",
    "        \"MODEL\": \"nearest_neighbour_matching\",\n",
    "        \"PARAMS\": {\n",
    "            \"treatment_column\": \"enriched\",\n",
    "            \"covariate_columns\": [\"price\"],\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "            \"caliper\": 0.2,\n",
    "            \"replace\": True,\n",
    "            \"ratio\": 1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def extract_te(results):\n",
    "    \"\"\"Extract the treatment effect from model results regardless of model type.\"\"\"\n",
    "    estimates = results[\"data\"][\"impact_estimates\"]\n",
    "    model_type = results[\"model_type\"]\n",
    "    if model_type == \"experiment\":\n",
    "        return estimates[\"params\"].get(\"enriched[T.True]\", estimates[\"params\"].get(\"enriched\", 0))\n",
    "    elif model_type == \"nearest_neighbour_matching\":\n",
    "        return estimates[\"att\"]\n",
    "    else:\n",
    "        return estimates[\"treatment_effect\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "model_estimates = {}\n",
    "\n",
    "for name, measurement in model_overrides.items():\n",
    "    job_id = measurement[\"MODEL\"]\n",
    "    results = run_with_override(base_config, measurement, str(output_path), job_id)\n",
    "    model_results[name] = results\n",
    "    model_estimates[name] = extract_te(results)\n",
    "    print(f\"{name}: treatment effect = {model_estimates[name]:.4f}\")\n",
    "\n",
    "print(f\"\\nTrue effect: {true_te:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"Estimate\": est,\n",
    "            \"True Effect\": true_te,\n",
    "            \"Absolute Error\": abs(est - true_te),\n",
    "            \"Recovery (%)\": max(0, (1 - abs(1 - est / true_te)) * 100) if true_te != 0 else 0,\n",
    "        }\n",
    "        for name, est in model_estimates.items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CROSS-SECTIONAL MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_support import plot_model_comparison\n",
    "\n",
    "plot_model_comparison(\n",
    "    model_names=list(model_estimates.keys()),\n",
    "    estimates=list(model_estimates.values()),\n",
    "    true_effect=true_te,\n",
    "    ylabel=\"Treatment Effect\",\n",
    "    title=\"Cross-Sectional Models: Estimates vs Truth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parameter Sensitivity\n",
    "\n",
    "For a given model and data, how sensitive is the treatment effect estimate to tuning parameters?\n",
    "We use the same override pattern, varying one parameter at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Subclassification: `n_strata`\n",
    "\n",
    "More strata means finer partitioning of the covariate space.\n",
    "This can improve precision but may leave strata without common support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_strata_values = [2, 3, 5, 10, 20, 50, 100]\n",
    "subclass_estimates = []\n",
    "strata_used = []\n",
    "strata_dropped = []\n",
    "\n",
    "for n in n_strata_values:\n",
    "    measurement = {\n",
    "        \"MODEL\": \"subclassification\",\n",
    "        \"PARAMS\": {\n",
    "            \"treatment_column\": \"enriched\",\n",
    "            \"covariate_columns\": [\"price\"],\n",
    "            \"n_strata\": n,\n",
    "            \"estimand\": \"att\",\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "        },\n",
    "    }\n",
    "    results = run_with_override(base_config, measurement, str(output_path), f\"subclass_strata_{n}\")\n",
    "    estimates = results[\"data\"][\"impact_estimates\"]\n",
    "\n",
    "    subclass_estimates.append(estimates[\"treatment_effect\"])\n",
    "    strata_used.append(estimates[\"n_strata\"])\n",
    "    strata_dropped.append(estimates[\"n_strata_dropped\"])\n",
    "\n",
    "subclass_sensitivity = pd.DataFrame(\n",
    "    {\n",
    "        \"n_strata (requested)\": n_strata_values,\n",
    "        \"Strata Used\": strata_used,\n",
    "        \"Strata Dropped\": strata_dropped,\n",
    "        \"Treatment Effect\": subclass_estimates,\n",
    "        \"Absolute Error\": [abs(est - true_te) for est in subclass_estimates],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Subclassification: n_strata Sensitivity\")\n",
    "print(\"-\" * 70)\n",
    "print(subclass_sensitivity.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_support import plot_parameter_sensitivity\n",
    "\n",
    "plot_parameter_sensitivity(\n",
    "    param_values=n_strata_values,\n",
    "    estimates=subclass_estimates,\n",
    "    true_effect=true_te,\n",
    "    xlabel=\"Number of Strata (n_strata)\",\n",
    "    ylabel=\"Treatment Effect\",\n",
    "    title=\"Subclassification: Sensitivity to n_strata\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Nearest Neighbour Matching: `caliper`\n",
    "\n",
    "The caliper controls the maximum allowed distance between a treated unit and its matched control.\n",
    "Smaller values enforce tighter matches but may discard units, while larger values allow more matches with worse balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caliper_values = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0]\n",
    "matching_estimates = []\n",
    "n_matched_att_list = []\n",
    "\n",
    "for cal in caliper_values:\n",
    "    measurement = {\n",
    "        \"MODEL\": \"nearest_neighbour_matching\",\n",
    "        \"PARAMS\": {\n",
    "            \"treatment_column\": \"enriched\",\n",
    "            \"covariate_columns\": [\"price\"],\n",
    "            \"dependent_variable\": \"revenue\",\n",
    "            \"caliper\": cal,\n",
    "            \"replace\": True,\n",
    "            \"ratio\": 1,\n",
    "        },\n",
    "    }\n",
    "    results = run_with_override(base_config, measurement, str(output_path), f\"matching_caliper_{cal}\")\n",
    "    estimates = results[\"data\"][\"impact_estimates\"]\n",
    "    summary = results[\"data\"][\"model_summary\"]\n",
    "\n",
    "    matching_estimates.append(estimates[\"att\"])\n",
    "    n_matched_att_list.append(summary[\"n_matched_att\"])\n",
    "\n",
    "matching_sensitivity = pd.DataFrame(\n",
    "    {\n",
    "        \"Caliper\": caliper_values,\n",
    "        \"N Matched (ATT)\": n_matched_att_list,\n",
    "        \"Treatment Effect (ATT)\": matching_estimates,\n",
    "        \"Absolute Error\": [abs(est - true_te) for est in matching_estimates],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Nearest Neighbour Matching: Caliper Sensitivity\")\n",
    "print(\"-\" * 70)\n",
    "print(matching_sensitivity.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_sensitivity(\n",
    "    param_values=caliper_values,\n",
    "    estimates=matching_estimates,\n",
    "    true_effect=true_te,\n",
    "    xlabel=\"Caliper\",\n",
    "    ylabel=\"Treatment Effect (ATT)\",\n",
    "    title=\"Nearest Neighbour Matching: Sensitivity to Caliper\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Model swappability.**\n",
    "- All three models recover the true treatment effect from the same simulated data.\n",
    "- Switching models requires only changing the `MEASUREMENT` entry in the config.\n",
    "- The `evaluate_impact()` interface stays the same regardless of the model.\n",
    "\n",
    "**Parameter sensitivity.**\n",
    "- **Subclassification** is relatively stable across `n_strata` values. Very low values may under-partition, while very high values may drop strata with insufficient common support.\n",
    "- **Nearest neighbour matching** is more sensitive to `caliper`. Very small calipers may discard too many units, while very large calipers degrade match quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
